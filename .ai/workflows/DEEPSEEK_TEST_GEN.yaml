name: "DeepSeek Test Generation"
description: "AI-powered test generation using DeepSeek Coder V2 Lite (local, free)"

inputs:
  files: ["src/module.py"]
  test_framework: "pytest"

policy:
  max_tokens: 0  # Free local inference
  prefer_deterministic: false  # Using AI for test generation
  timeout_minutes: 15

steps:
  - id: "1.001"
    name: "Analyze Code Coverage"
    actor: pytest_runner
    with:
      path: "."
      coverage: true
      coverage_report: true
    emits: ["artifacts/coverage_report.json"]

  - id: "1.002"
    name: "Identify Coverage Gaps"
    actor: deepseek
    with:
      tool: ollama_direct
      operation: analyze
      prompt: |
        Analyze the coverage report and identify:
        1. Functions/methods with no tests
        2. Edge cases not covered
        3. Error handling paths not tested
        4. Integration points needing tests

        Provide a prioritized list of test cases to write.
      max_tokens: 2000
    emits: ["artifacts/test_gaps.json"]

  - id: "1.003"
    name: "Generate Tests with DeepSeek"
    actor: deepseek
    with:
      tool: aider
      operation: generate
      read_only: false
      prompt: |
        Generate comprehensive pytest tests for the target files. Include:

        1. Unit Tests:
           - Happy path scenarios
           - Edge cases
           - Error conditions
           - Boundary values

        2. Test Structure:
           - Fixtures for setup/teardown
           - Parameterized tests for multiple inputs
           - Mocking external dependencies
           - Clear test names and docstrings

        3. Best Practices:
           - Arrange-Act-Assert pattern
           - One assertion per test (where appropriate)
           - Fast, isolated tests
           - Good test coverage

        Create tests in tests/ directory with test_ prefix.
    emits: ["artifacts/generated_tests_diff.json"]

  - id: "1.004"
    name: "Run Generated Tests"
    actor: pytest_runner
    with:
      path: "tests/"
      coverage: true
      verbose: true
    emits: ["artifacts/new_test_report.json"]

  - id: "1.005"
    name: "Verify Tests Pass"
    actor: verifier
    with:
      checks:
        - type: "tests_pass"
          from: "artifacts/new_test_report.json"
    on_fail: "pause_for_review"

  - id: "1.006"
    name: "Review Test Quality"
    actor: deepseek
    with:
      tool: ollama_direct
      operation: review
      prompt: |
        Review the generated tests for:
        1. Completeness - are all scenarios covered?
        2. Quality - are tests well-structured?
        3. Maintainability - are tests easy to understand?
        4. Performance - do tests run quickly?

        Provide specific feedback and suggestions for improvement.
      max_tokens: 2000
    emits: ["artifacts/test_review.md"]

  - id: "1.007"
    name: "Git Commit Tests"
    actor: git_ops
    with:
      branch: "lane/tests/deepseek-gen"
      commit_message_template: "test: add comprehensive tests generated by DeepSeek\n\n{summary}"
      create_pr: true
    emits: ["artifacts/pr_url.txt"]
